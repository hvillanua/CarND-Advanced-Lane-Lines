{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline\n",
    "from moviepy.editor import VideoFileClip, ImageSequenceClip\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(helper)\n",
    "\n",
    "cal_path = './camera_cal/'\n",
    "output_images = './output_images/'\n",
    "path = './test_images/'\n",
    "\n",
    "def pipeline(img, mtx, dist, roi_vertices, M, M_inv, left_line, right_line, bad_frames):\n",
    "    # Undistort image\n",
    "    undist = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    imshape = undist.shape\n",
    "\n",
    "    # Apply thresholds to image and combine them\n",
    "    # Choose a Sobel kernel size\n",
    "    ksize = 5\n",
    "    \n",
    "    # Convert to HLS color space and separate the S channel\n",
    "    hls = cv2.cvtColor(undist, cv2.COLOR_RGB2HLS)\n",
    "    h_channel = hls[...,0]\n",
    "    h_channel = helper.clahe(h_channel, 4, (5, 5))\n",
    "    l_channel = hls[...,1]\n",
    "    l_channel = helper.clahe(l_channel, 6, (15, 15))\n",
    "    s_channel = hls[...,2]\n",
    "    s_channel = helper.clahe(s_channel, 4, (5, 5))\n",
    "\n",
    "    # Apply thresholds to image and combine them\n",
    "    # Choose a Sobel kernel size\n",
    "    ksize = 7\n",
    "    close_kernel = np.ones((5,5),np.uint8)\n",
    "\n",
    "    gradx = helper.abs_sobel_thresh(s_channel, orient='x', kernel=ksize,\n",
    "                                    thresh=(50, 200))\n",
    "    gradx = cv2.morphologyEx(gradx, cv2.MORPH_CLOSE, kernel=close_kernel)\n",
    "    grady = helper.abs_sobel_thresh(s_channel, orient='y', kernel=ksize,\n",
    "                                    thresh=(70, 200))\n",
    "    mag_binary = helper.mag_thresh(s_channel, kernel=ksize, thresh=(30, 200))\n",
    "    gradients = np.zeros_like(gradx)\n",
    "    gradients[((gradx == 1) | (grady == 1)) & (mag_binary == 1)] = 1\n",
    "    \n",
    "    yellow_min = np.array([15, 60, 60], np.uint8)\n",
    "    yellow_max = np.array([90, 200, 255], np.uint8)\n",
    "    yellow_mask = cv2.inRange(hls, yellow_min, yellow_max)\n",
    "    \n",
    "    white_min = np.array([0, 200, 0], np.uint8)\n",
    "    white_max = np.array([255, 255, 255], np.uint8)\n",
    "    white_mask = cv2.inRange(hls, white_min, white_max)\n",
    "\n",
    "    binary_output = np.zeros_like(gradients)\n",
    "    binary_output[((yellow_mask == 255) | (white_mask == 255))] = 1\n",
    "\n",
    "    color_binary_h = helper.color_threshold(h_channel, thresh=(30, 80))\n",
    "    color_binary_l = helper.color_threshold(l_channel, thresh=(170, 255))\n",
    "    color_binary_s = helper.color_threshold(s_channel, thresh=(100, 255))\n",
    "\n",
    "    color_combined = np.zeros_like(binary_output)\n",
    "    color_combined[(color_binary_h == 1) | ((color_binary_s == 1) | (color_binary_l == 1))] = 1\n",
    "\n",
    "    # Combine gradient and color thresholding\n",
    "    mask = np.zeros_like(binary_output)\n",
    "    mask[(binary_output == 1) & ((gradients == 1) | (color_combined == 1))] = 1\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel=close_kernel)\n",
    "\n",
    "    roi = helper.region_of_interest(mask, [roi_vertices])\n",
    "    warped = cv2.warpPerspective(roi, M, roi.shape[::-1])\n",
    "\n",
    "    # Convolution over warped image to find hot pixels defining lane lines\n",
    "    conv_kernel = np.ones((5,5),np.uint8)\n",
    "    conv = helper.conv_img(warped, conv_kernel)\n",
    "    \n",
    "    if (not left_line.detected or not right_line.detected) and bad_frames > 2 or \\\n",
    "    left_line.best_fit is None or right_line.best_fit is None:\n",
    "        # Calculate initial lane centroids\n",
    "        initial_fit = True\n",
    "        left_x_centroids, left_y_centroids, \\\n",
    "        right_x_centroids, right_y_centroids = helper.window_search(conv) \n",
    "    else:\n",
    "        # Calculate lane centroids using previous ones as support\n",
    "        left_x_centroids, left_y_centroids = helper.guided_window_search(conv, left_line)\n",
    "        right_x_centroids, right_y_centroids = helper.guided_window_search(conv, right_line)\n",
    "\n",
    "    \n",
    "    # Calculate base position and update lines\n",
    "    ym_per_pix = 30 / 720\n",
    "    xm_per_pix = 3.7 / 790\n",
    "\n",
    "    halfway = 3.7 / 2\n",
    "    car_pos = conv.shape[1] / 2\n",
    "    if left_line.best_fit is None or right_line.best_fit is None:\n",
    "        left_base_pos = None\n",
    "        right_base_pos = None\n",
    "        line_base_pos = 0\n",
    "    else:\n",
    "        left_pos = np.polyval(left_line.best_fit, conv.shape[1])\n",
    "        right_pos = np.polyval(right_line.best_fit, conv.shape[1])\n",
    "        lane_px = abs(left_pos - right_pos)\n",
    "        lane_midpoint = lane_px / 2 + left_pos\n",
    "        line_base_pos = (car_pos - lane_midpoint) * xm_per_pix\n",
    "        left_base_pos = halfway + line_base_pos\n",
    "        right_base_pos = halfway - line_base_pos\n",
    "\n",
    "    # Update lines\n",
    "    left_line = helper.update_line(conv, left_line, left_x_centroids, \\\n",
    "                                   left_y_centroids, left_base_pos)\n",
    "\n",
    "    right_line = helper.update_line(conv, right_line, right_x_centroids, \\\n",
    "                                    right_y_centroids, right_base_pos)\n",
    "    \n",
    "    # Unwarp image\n",
    "    ploty = np.linspace(0, conv.shape[0]-1, conv.shape[0])\n",
    "    left_fitx = np.polyval(left_line.best_fit, ploty)\n",
    "    right_fitx = np.polyval(right_line.best_fit, ploty)\n",
    "    left_zipped = np.array(list(zip(left_fitx, ploty)), dtype=np.int32)\n",
    "    right_zipped = np.array(list(zip(right_fitx, ploty)), dtype=np.int32)\n",
    "    \n",
    "    fitted = np.zeros_like(conv)\n",
    "    fitted = np.dstack((fitted, fitted, fitted))\n",
    "    cv2.fillPoly(fitted, [np.concatenate((left_zipped, right_zipped[::-1]))],\n",
    "             (0,255, 0))\n",
    "    \n",
    "    # Uncomment to get a video side by side with the top-view of the lane\n",
    "    # with the algorithm working\n",
    "    \n",
    "#     aux_fitted = np.zeros_like(fitted)\n",
    "    \n",
    "#     cv2.polylines(aux_fitted, [left_zipped], False, (0,0,255), 6, -1)\n",
    "#     cv2.polylines(aux_fitted, [right_zipped], False, (0,0,255), 6, -1)\n",
    "    \n",
    "#     for values in left_line.recent_centroids_fitted:\n",
    "#         for centroid in values:\n",
    "#             cv2.circle(aux_fitted, (int(centroid[0]), int(centroid[1])),\n",
    "#                        6, (0,255,0), -1)\n",
    "#     for values in right_line.recent_centroids_fitted:\n",
    "#         for centroid in values:\n",
    "#             cv2.circle(aux_fitted, (int(centroid[0]), int(centroid[1])),\n",
    "#                        6, (0,255,0), -1)\n",
    "    \n",
    "    unwarped = cv2.warpPerspective(fitted, M_inv, fitted.shape[1::-1])\n",
    "    \n",
    "    curverad = (left_line.radius_of_curvature + right_line.radius_of_curvature) / 2\n",
    "    curverad = round(curverad, 2)\n",
    "    cv2.putText(unwarped, 'Radius of curvature: ' + str(curverad) + 'm',\n",
    "                (40, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255))\n",
    "\n",
    "    line_base_pos = round(line_base_pos, 2)\n",
    "    if line_base_pos < 0:\n",
    "        cv2.putText(unwarped, 'Base position: ' + str(-line_base_pos) + \n",
    "                    'm left of center', (40, 60), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                    (255,255,255))\n",
    "    else:\n",
    "        cv2.putText(unwarped, 'Base position: ' + str(line_base_pos) + \n",
    "                    'm right of center', (40, 60), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                    (255,255,255))\n",
    "    \n",
    "    weighted = cv2.addWeighted(unwarped, 0.4, undist, 1., 0.)\n",
    "    \n",
    "    # Uncomment to get a video side by side with the top-view of the lane\n",
    "    # with the algorithm working\n",
    "    \n",
    "#     aux_conv = np.dstack((conv, conv, conv))*128\n",
    "#     aux_weighted = cv2.addWeighted(aux_fitted, 0.4, aux_conv, 1., 0.)\n",
    "#     weighted = np.hstack((weighted, aux_weighted))\n",
    "    \n",
    "    return weighted, left_line, right_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtx, dist = helper.camera_calibration(cal_path, output_images)\n",
    "\n",
    "bad_frames = 0\n",
    "output_path = './output.mp4'\n",
    "# input_video = './project_video.mp4'\n",
    "input_video = './challenge_video.mp4'\n",
    "clip = VideoFileClip(input_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame = clip.get_frame(0)\n",
    "imshape = frame.shape\n",
    "low_y = imshape[0] - 50\n",
    "high_y = imshape[0] / 2 + 72\n",
    "\n",
    "if input_video == './project_video.mp4':\n",
    "    src_vertices = np.array([(565, 470),\n",
    "                          (275, 670),\n",
    "                          (1035, 670),\n",
    "                          (720, 470),\n",
    "                         ])\n",
    "    roi_vertices = np.array([(220,low_y),\n",
    "                      (imshape[1]/2-25, high_y), \n",
    "                      (imshape[1]/2+60, high_y), \n",
    "                      (imshape[1]-130,low_y)\n",
    "                     ], dtype=np.int32)\n",
    "    \n",
    "else:\n",
    "    src_vertices = np.array([(570, 510),\n",
    "                              (320, 690),\n",
    "                              (1090, 690),\n",
    "                              (790, 510),\n",
    "                             ])\n",
    "    roi_vertices = np.array([(280,low_y),\n",
    "                      (imshape[1]/2-30, high_y+40), \n",
    "                      (imshape[1]/2+110, high_y+40), \n",
    "                      (imshape[1]-170,low_y)\n",
    "                     ], dtype=np.int32)\n",
    "    \n",
    "dst_vertices = np.array([(260, 0),\n",
    "                         (260, imshape[0]),\n",
    "                         (1050, imshape[0]),\n",
    "                         (1050, 0),\n",
    "                        ])\n",
    "\n",
    "# Calculate perspective transform and inverse transform\n",
    "M, M_inv = helper.perspective_transform(src_vertices, dst_vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video ./output.mp4\n",
      "[MoviePy] Writing video ./output.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 485/485 [00:11<00:00, 42.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: ./output.mp4 \n",
      "\n",
      "CPU times: user 7.49 s, sys: 316 ms, total: 7.81 s\n",
      "Wall time: 12.1 s\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(helper)\n",
    "\n",
    "left_line = helper.Line()\n",
    "right_line = helper.Line()\n",
    "out_frames = []\n",
    "loops = 0\n",
    "\n",
    "temp_dir = './temp/'\n",
    "if not os.path.isdir(temp_dir):\n",
    "    os.mkdir(temp_dir)\n",
    "\n",
    "for frame in clip.iter_frames():\n",
    "\n",
    "    new_frame, left_line, right_line = pipeline(frame, mtx, dist, roi_vertices,\n",
    "                                                M, M_inv, left_line, right_line,\n",
    "                                                bad_frames)\n",
    "    if not left_line.detected or not right_line.detected:\n",
    "        bad_frames += 1\n",
    "    else:\n",
    "        bad_frames = 0\n",
    "\n",
    "    frame_name = temp_dir + 'temp_'+str(loops)+'.jpg'\n",
    "    cv2.imwrite(frame_name, cv2.cvtColor(new_frame, cv2.COLOR_RGB2BGR))\n",
    "    loops += 1\n",
    "    out_frames.append(frame_name)\n",
    "\n",
    "out_clip = ImageSequenceClip(out_frames, fps=clip.fps)\n",
    "%time out_clip.write_videofile(output_path, audio=False)\n",
    "# Comment the line below to keep the images generated by the pipeline\n",
    "shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_path = path + 'straight_lines1.jpg'\n",
    "image = cv2.imread(img_path)\n",
    "undist = cv2.undistort(image, mtx, dist, None, mtx)\n",
    "cv2.imwrite(output_images+'undistorted.jpg', undist)\n",
    "hls = cv2.cvtColor(undist, cv2.COLOR_BGR2HLS)\n",
    "\n",
    "h_channel = hls[...,0]\n",
    "h_channel = helper.clahe(h_channel, 4, (5, 5))\n",
    "l_channel = hls[...,1]\n",
    "l_channel = helper.clahe(l_channel, 6, (15, 15))\n",
    "s_channel = hls[...,2]\n",
    "s_channel = helper.clahe(s_channel, 4, (5, 5))\n",
    "\n",
    "cv2.imwrite(output_images+'h_channel.jpg', h_channel)\n",
    "cv2.imwrite(output_images+'l_channel.jpg', l_channel)\n",
    "cv2.imwrite(output_images+'s_channel.jpg', s_channel);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply thresholds to image and combine them\n",
    "# Choose a Sobel kernel size\n",
    "ksize = 7\n",
    "close_kernel = np.ones((5,5),np.uint8)\n",
    "\n",
    "gradx = helper.abs_sobel_thresh(s_channel, orient='x', kernel=ksize, thresh=(50, 200))\n",
    "gradx = cv2.morphologyEx(gradx, cv2.MORPH_CLOSE, kernel=close_kernel)\n",
    "cv2.imwrite(output_images+'x_gradient.jpg', gradx*255)\n",
    "\n",
    "grady = helper.abs_sobel_thresh(s_channel, orient='y', kernel=ksize, thresh=(70, 200))\n",
    "cv2.imwrite(output_images+'y_gradient.jpg', grady*255)\n",
    "\n",
    "mag_binary = helper.mag_thresh(s_channel, kernel=ksize, thresh=(30, 200))\n",
    "cv2.imwrite(output_images+'gradient_magnitude.jpg', mag_binary*255)\n",
    "\n",
    "gradients = np.zeros_like(gradx)\n",
    "gradients[((gradx == 1) | (grady == 1)) & (mag_binary == 1)] = 1\n",
    "cv2.imwrite(output_images+'combined_gradients.jpg', gradients*255);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yellow_min = np.array([15, 60, 60], np.uint8)\n",
    "yellow_max = np.array([90, 200, 255], np.uint8)\n",
    "yellow_mask = cv2.inRange(hls, yellow_min, yellow_max)\n",
    "cv2.imwrite(output_images+'yellow_mask.jpg', yellow_mask)\n",
    "\n",
    "white_min = np.array([0, 200, 0], np.uint8)\n",
    "white_max = np.array([255, 255, 255], np.uint8)\n",
    "white_mask = cv2.inRange(hls, white_min, white_max)\n",
    "cv2.imwrite(output_images+'white_mask.jpg', white_mask)\n",
    "\n",
    "binary_output = np.zeros_like(gradients)\n",
    "binary_output[((yellow_mask == 255) | (white_mask == 255))] = 1\n",
    "cv2.imwrite(output_images+'yellow_white_mask.jpg', binary_output*255)\n",
    "\n",
    "color_binary_h = helper.color_threshold(h_channel, thresh=(30, 80))\n",
    "cv2.imwrite(output_images+'color_thresh_h.jpg', color_binary_h*255)\n",
    "color_binary_l = helper.color_threshold(l_channel, thresh=(170, 255))\n",
    "cv2.imwrite(output_images+'color_thresh_l.jpg', color_binary_l*255)\n",
    "color_binary_s = helper.color_threshold(s_channel, thresh=(100, 255))\n",
    "cv2.imwrite(output_images+'color_thresh_s.jpg', color_binary_s*255)\n",
    "\n",
    "color_combined = np.zeros_like(gradients)\n",
    "color_combined[(color_binary_h == 1) & ((color_binary_s == 1) | (color_binary_l == 1))] = 1\n",
    "cv2.imwrite(output_images+'channels_thresh_combined.jpg', color_combined*255)\n",
    "\n",
    "# Combine gradient and color thresholding\n",
    "mask = np.zeros_like(gradients)\n",
    "mask[((gradients == 1) | (color_combined == 1)) & (binary_output == 1)] = 1\n",
    "mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel=close_kernel)\n",
    "cv2.imwrite(output_images+'final_thresh.jpg', mask*255)\n",
    "\n",
    "color_binary = np.dstack((binary_output, color_combined, gradients))*255\n",
    "cv2.imwrite(output_images+'grad_color_thresh.jpg', color_binary);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image3 = np.copy(image)\n",
    "imshape = image.shape\n",
    "low_y = imshape[0] - 30\n",
    "high_y = imshape[0] / 2 + 72\n",
    "roi_vertices = np.array([(220,low_y),\n",
    "                      (imshape[1]/2-25, high_y), \n",
    "                      (imshape[1]/2+60, high_y), \n",
    "                      (imshape[1]-130,low_y)\n",
    "                     ], dtype=np.int32)\n",
    "\n",
    "cv2.polylines(image3, [roi_vertices], True, (255,0,0), 3)\n",
    "cv2.imwrite(output_images+'roi.jpg', image3)\n",
    "\n",
    "roi = helper.region_of_interest(mask, [roi_vertices])\n",
    "cv2.imwrite(output_images+'masked_roi.jpg', roi*255);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_vertices = np.array([(565, 470),\n",
    "                          (275, 670),\n",
    "                          (1035, 670),\n",
    "                          (720, 470),\n",
    "                         ])\n",
    "\n",
    "dst_vertices = np.array([(260, 0),\n",
    "                         (260, imshape[0]),\n",
    "                         (1050, imshape[0]),\n",
    "                         (1050, 0),\n",
    "                        ])\n",
    "\n",
    "prueba = np.copy(image)\n",
    "for src in src_vertices:\n",
    "    cv2.circle(prueba, tuple(src), 10, (255,0,0), -1)\n",
    "    \n",
    "cv2.imwrite(output_images+'perspective_vertices.jpg', prueba)\n",
    "\n",
    "# Calculate perspective transform and inverse transform\n",
    "M, M_inv = helper.perspective_transform(src_vertices, dst_vertices)\n",
    "wrp = cv2.warpPerspective(image, M, roi.shape[::-1])\n",
    "\n",
    "# Warp image\n",
    "warped = cv2.warpPerspective(roi, M, roi.shape[::-1])\n",
    "cv2.imwrite(output_images+'warped.jpg', warped*255)\n",
    "\n",
    "# Compute convolution over warped image to find hot pixels defining lane lines\n",
    "conv_kernel = np.ones((5,5),np.uint8)\n",
    "conv = helper.conv_img(warped, conv_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "left_x_centroids, left_y_centroids, \\\n",
    "right_x_centroids, right_y_centroids = helper.window_search(conv, verbose=True, img_path=output_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit lines\n",
    "left_fit = np.polyfit(left_y_centroids, left_x_centroids, 2)\n",
    "right_fit = np.polyfit(right_y_centroids, right_x_centroids, 2)\n",
    "\n",
    "ploty = np.linspace(0, conv.shape[0]-1, conv.shape[0])\n",
    "left_fitx = np.polyval(left_fit, ploty)\n",
    "right_fitx = np.polyval(right_fit, ploty)\n",
    "\n",
    "fitted_conv = np.copy(conv)*255\n",
    "fitted_conv = np.dstack((np.copy(fitted_conv), np.copy(fitted_conv),\n",
    "                         np.copy(fitted_conv)))\n",
    "\n",
    "# fitted = np.zeros_like(conv)\n",
    "left_zipped = np.array(list(zip(left_fitx, ploty)), dtype=np.int32)\n",
    "right_zipped = np.array(list(zip(right_fitx, ploty)), dtype=np.int32)\n",
    "cv2.fillPoly(fitted_conv, [np.concatenate((left_zipped, right_zipped[::-1]))], (0,255, 0))\n",
    "\n",
    "# Unwarp image\n",
    "unwarped = cv2.warpPerspective(fitted_conv, M_inv, conv.shape[::-1])\n",
    "cv2.imwrite(output_images+'fitted_warped.jpg', fitted_conv);\n",
    "cv2.imwrite(output_images+'fitted_unwarped.jpg', unwarped);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
